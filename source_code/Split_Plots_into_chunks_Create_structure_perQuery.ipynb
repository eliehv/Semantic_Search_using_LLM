{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl9RKU8dUhTK"
      },
      "outputs": [],
      "source": [
        "##########################################################################################\n",
        "#####             Split Plot to smaller chunk with lenght of max token = 50\n",
        "##########################################################################################\n",
        "max_tokens = 50\n",
        "# Function to split the text into chunks of a maximum number of tokens\n",
        "def split_into_many(text, max_tokens = max_tokens):\n",
        "    # Split the text into sentences\n",
        "    #print('\\n**************', 'in split to many ', '******************\\n')\n",
        "    sentences = text.split('. ')\n",
        "    # Get the number of tokens for each sentence\n",
        "    #n_tokens = [len(tokenizer(\" \" + sentence, padding=True, truncation=False, return_tensors='pt')[0]) for sentence in sentences]\n",
        "    n_tokens = [len([token.text for token in nlp(sentence)]) for sentence in sentences]\n",
        "    chunks = []\n",
        "    tokens_so_far = 0\n",
        "    chunk = []\n",
        "    # Loop through the sentences and tokens joined together in a tuple\n",
        "    for sentence, token in zip(sentences, n_tokens):\n",
        "        # If the number of tokens so far plus the number of tokens in the current sentence is greater\n",
        "        # than the max number of tokens, then add the chunk to the list of chunks and reset the chunk and tokens so far\n",
        "        if tokens_so_far + token > max_tokens:\n",
        "            chunks.append(\". \".join(chunk) + \".\")\n",
        "            chunk = []\n",
        "            tokens_so_far = 0\n",
        "        # If the number of tokens in the current sentence is greater than the max number of\n",
        "        # tokens, go to the next sentence\n",
        "        if token > max_tokens:\n",
        "            continue\n",
        "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
        "        chunk.append(sentence)\n",
        "        tokens_so_far += token + 1\n",
        "    # Add the last chunk to the list of chunks\n",
        "    if chunk:\n",
        "        chunks.append(\". \".join(chunk) + \".\")\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################################################\n",
        "#####                             Clean text to the word tokens\n",
        "##########################################################################################\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "def cleanup_text(docs):\n",
        "    texts = []\n",
        "    #counter = 1\n",
        "    for doc in docs:\n",
        "      #counter += 1\n",
        "      doc = nlp(doc)# nlp(doc,disable=['parser', 'ner'])\n",
        "      tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-' and not tok.is_stop and not tok.like_num and  not tok.is_punct]\n",
        "      tokens = ' '.join(tokens)\n",
        "      texts.append(tokens)\n",
        "    return texts"
      ],
      "metadata": {
        "id": "4DIZV2gbUjI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################################################\n",
        "#####        create proper Structure for each Query containing quey, Top_k, chunks of plots\n",
        "######################################################################################################\n",
        "query_info = []\n",
        "for q in range(len(queries)):\n",
        "  query_json = {}\n",
        "  chunks_top_k = []\n",
        "  query_json.update({\"query\": queries[q],\"top_k\": {\"score_idx\": all_q_results[q], \"chunks\": []}})\n",
        "  chunks = []\n",
        "  for r in all_q_results[q][1]:\n",
        "    # If the text is None, go to the next row\n",
        "    if df_new.iloc[int(r)]['Plot'] is None:\n",
        "        continue\n",
        "    chunks = split_into_many(df_new.iloc[int(r)]['Plot'])\n",
        "    clean_text = cleanup_text(chunks)\n",
        "    chunks_top_k.append(clean_text)\n",
        "  query_json[\"top_k\"][\"chunks\"]=chunks_top_k\n",
        "  query_info.append(query_json)"
      ],
      "metadata": {
        "id": "yeouJ1CBUk_-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}